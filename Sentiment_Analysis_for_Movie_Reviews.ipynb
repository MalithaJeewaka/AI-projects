{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVylPXo5/CSHCjBLcW8aGg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3YfMaoXwCROK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296d6789-0878-4175-f676-a7aa2f55e152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First 5 rows of the dataset: ---\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "--- Dataset Information: ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "\n",
            "--- Sentiment Distribution: ---\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# --- Step 1: Import Libraries and Load Data ---\n",
        "\n",
        "# 'pandas' is a powerful library for data manipulation and analysis. We use it to load and handle our dataset.\n",
        "# We give it the shorter alias 'pd' by convention.\n",
        "import pandas as pd\n",
        "\n",
        "# 're' is Python's built-in library for regular expressions. We'll use it for cleaning text data.\n",
        "import re\n",
        "\n",
        "# 'nltk' (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data.\n",
        "import nltk\n",
        "\n",
        "# We need to download a specific resource from NLTK: 'stopwords'.\n",
        "# Stopwords are common words (like \"the\", \"a\", \"in\") that often don't carry significant meaning and can be removed.\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# --- Load the Dataset ---\n",
        "# We use pandas' read_csv function to load our data from the uploaded file.\n",
        "# The file is now a pandas DataFrame, which is like a spreadsheet or a table in memory.\n",
        "df = pd.read_csv('IMDB Dataset.csv')\n",
        "\n",
        "# --- Initial Exploration ---\n",
        "# Let's look at the first 5 rows of our data to understand its structure.\n",
        "# We should see two columns: 'review' (the text of the movie review) and 'sentiment' (positive/negative).\n",
        "print(\"--- First 5 rows of the dataset: ---\")\n",
        "print(df.head())\n",
        "\n",
        "# Let's get some basic information about our dataset.\n",
        "# .info() tells us the number of entries, column names, and data types.\n",
        "print(\"\\n--- Dataset Information: ---\")\n",
        "df.info()\n",
        "\n",
        "# Let's check the distribution of sentiments. A balanced dataset is good for training.\n",
        "print(\"\\n--- Sentiment Distribution: ---\")\n",
        "print(df['sentiment'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Preprocess the Text Data ---\n",
        "\n",
        "# Get the list of English stop words from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    This function takes a raw text string and performs the following cleaning steps:\n",
        "    1. Removes HTML tags.\n",
        "    2. Converts text to lowercase.\n",
        "    3. Removes punctuation and special characters.\n",
        "    4. Removes stop words.\n",
        "    \"\"\"\n",
        "    # 1. Remove HTML tags using a regular expression.\n",
        "    # re.sub finds a pattern and replaces it with another string. Here, '<.*?>' finds any sequence in angle brackets.\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # 2. Remove punctuation and non-alphabetic characters, then convert to lowercase.\n",
        "    # '[^a-zA-Z]' matches any character that is NOT a letter. We replace them with a space.\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()\n",
        "\n",
        "    # 3. Tokenize the text (split it into a list of words) and remove stop words.\n",
        "    # .split() breaks the string into a list of words based on spaces.\n",
        "    words = text.split()\n",
        "    # We use a list comprehension for an efficient loop. It creates a new list containing only the words\n",
        "    # that are NOT in our stop_words set.\n",
        "    clean_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # 4. Join the cleaned words back into a single string.\n",
        "    return ' '.join(clean_words)\n",
        "\n",
        "# Now, we apply our cleaning function to every review in the 'review' column.\n",
        "# The .apply() method is a powerful way to run a function on every item in a pandas Series (a column).\n",
        "# We create a new column 'cleaned_review' to store the result.\n",
        "print(\"\\n--- Cleaning the text data... (This may take a moment) ---\")\n",
        "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "print(\"--- Cleaning complete! ---\")\n",
        "\n",
        "# Let's look at an original review and its cleaned version.\n",
        "print(\"\\n--- Example of Cleaning ---\")\n",
        "print(\"Original Review:\", df['review'][0])\n",
        "print(\"\\nCleaned Review:\", df['cleaned_review'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO28otPaivEj",
        "outputId": "950cb347-e4df-40bd-f444-12f13ae81daa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cleaning the text data... (This may take a moment) ---\n",
            "--- Cleaning complete! ---\n",
            "\n",
            "--- Example of Cleaning ---\n",
            "Original Review: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "\n",
            "Cleaned Review: one reviewers mentioned watching oz episode hooked right exactly happened first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish scuffles death stares dodgy dealings shady agreements never far away would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 3: Vectorize the Text using TF-IDF ---\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# TfidfVectorizer will convert our collection of text documents into a matrix of TF-IDF features.\n",
        "# max_features=5000 means we'll only consider the top 5000 most frequent words to build our vocabulary.\n",
        "# This helps save memory and can prevent our model from overfitting on rare words.\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# We define our features (X) and our target (y).\n",
        "# X is the numerical representation of our cleaned reviews.\n",
        "# y is the sentiment label we want to predict.\n",
        "X = vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
        "y = df['sentiment']\n",
        "\n",
        "# Let's check the shape of our feature matrix X.\n",
        "# It should be (50000, 5000) -> 50000 reviews, 5000 features (words) each.\n",
        "print(\"\\n--- Shape of TF-IDF feature matrix (X): ---\")\n",
        "print(X.shape)\n",
        "\n",
        "# Our target 'y' is currently text ('positive', 'negative'). Models need numerical labels.\n",
        "# LabelEncoder converts 'positive' to 1 and 'negative' to 0.\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Let's see the mapping: 0 for negative, 1 for positive.\n",
        "print(\"\\n--- Sentiment Label Mapping: ---\")\n",
        "print(f\"'{label_encoder.classes_[0]}' is encoded as 0\")\n",
        "print(f\"'{label_encoder.classes_[1]}' is encoded as 1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nfQp0MIj5Dr",
        "outputId": "4000a7a2-2d4d-4e88-bd9e-fadd23140227"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Shape of TF-IDF feature matrix (X): ---\n",
            "(50000, 5000)\n",
            "\n",
            "--- Sentiment Label Mapping: ---\n",
            "'negative' is encoded as 0\n",
            "'positive' is encoded as 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 4: Split Data and Train the Model ---\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# We split our data: 80% for training and 20% for testing.\n",
        "# X_train, y_train will be used to teach the model.\n",
        "# X_test, y_test will be used to see how well it learned.\n",
        "# random_state=42 ensures that we get the same split every time we run the code, for reproducibility.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Logistic Regression model.\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model using our training data.\n",
        "# The .fit() method is the \"learning\" step. The model learns the relationship\n",
        "# between the TF-IDF features (X_train) and the sentiment labels (y_train).\n",
        "print(\"\\n--- Training the Logistic Regression model... ---\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"--- Model training complete! ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glBmVZ00lbzU",
        "outputId": "bb6fdbfb-aa5d-483f-c556-40524455eef8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training the Logistic Regression model... ---\n",
            "--- Model training complete! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 5: Evaluate the Model ---\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Ask the trained model to make predictions on the unseen test data.\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n--- Model Accuracy: {accuracy * 100:.2f}% ---\")\n",
        "\n",
        "# Print a detailed classification report.\n",
        "# Precision: Of all the reviews we predicted as positive, how many were actually positive?\n",
        "# Recall: Of all the actual positive reviews, how many did we correctly identify?\n",
        "# F1-Score: A balanced measure of precision and recall.\n",
        "print(\"\\n--- Classification Report: ---\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRK3_qAhlj0h",
        "outputId": "c2860d3b-7b6a-45dd-a173-10ff67809996"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Accuracy: 89.22% ---\n",
            "\n",
            "--- Classification Report: ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.88      0.89      4961\n",
            "    positive       0.88      0.91      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 6: Use the Model for Prediction on New Data ---\n",
        "\n",
        "# Let's write some new reviews.\n",
        "new_reviews = [\n",
        "    \"This movie is good for blind people\",\n",
        "    \"What a waste of time. The plot was predictable and the acting was terrible. I would not recommend it.\",\n",
        "    \"The film was okay, not great but not bad either. Some interesting scenes.\"\n",
        "]\n",
        "\n",
        "print(\"\\n--- Predicting sentiment for new reviews: ---\")\n",
        "\n",
        "# 1. Clean the new reviews using our preprocessing function.\n",
        "cleaned_new_reviews = [preprocess_text(review) for review in new_reviews]\n",
        "\n",
        "# 2. Convert the cleaned reviews into a TF-IDF matrix.\n",
        "# IMPORTANT: We use vectorizer.transform() here, NOT .fit_transform().\n",
        "# We want to use the same vocabulary the model was trained on. 'fit' learns the vocabulary, 'transform' applies it.\n",
        "new_reviews_tfidf = vectorizer.transform(cleaned_new_reviews)\n",
        "\n",
        "# 3. Make predictions using our trained model.\n",
        "new_predictions = model.predict(new_reviews_tfidf)\n",
        "\n",
        "# 4. Convert the numerical predictions (0 or 1) back to text labels ('negative' or 'positive').\n",
        "new_predictions_labels = label_encoder.inverse_transform(new_predictions)\n",
        "\n",
        "# Display the results\n",
        "for review, sentiment in zip(new_reviews, new_predictions_labels):\n",
        "    print(f\"\\nReview: '{review}'\")\n",
        "    print(f\"Predicted Sentiment: {sentiment.upper()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-Ep3FeclrHj",
        "outputId": "2bd7cfc9-f2bb-44ca-e772-1d3255c1dc75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Predicting sentiment for new reviews: ---\n",
            "\n",
            "Review: 'This movie is good for blind people'\n",
            "Predicted Sentiment: POSITIVE\n",
            "\n",
            "Review: 'What a waste of time. The plot was predictable and the acting was terrible. I would not recommend it.'\n",
            "Predicted Sentiment: NEGATIVE\n",
            "\n",
            "Review: 'The film was okay, not great but not bad either. Some interesting scenes.'\n",
            "Predicted Sentiment: NEGATIVE\n"
          ]
        }
      ]
    }
  ]
}